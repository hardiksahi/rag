{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a02c77c-b776-4445-8d79-c75b956e06e6",
   "metadata": {},
   "source": [
    "Follows steps in https://www.youtube.com/watch?v=sVcwVQRHIc8&t=2s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397722b9-d6ba-40d7-85d5-4e3c123cb08e",
   "metadata": {},
   "source": [
    "Part 1: https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_1_to_4.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62162c47-10aa-42f5-b9e1-47c36f933298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tiktoken._educational import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd141b6-b7af-446e-ad05-21d067de9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e0afcd-2b23-4472-84f2-6290d2f0ef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c92304f-a84e-4179-aa82-a5ba5784a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader, YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7351099a-ddb1-4fc2-b1eb-0d65adc84410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f52546-13bf-48e3-b6cb-6bb475b09749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ef917c-2c3b-4c0c-9424-219223928738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46515cc1-1eb5-4e50-883b-6123a9804903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fbab9e-f847-4ed5-8999-276086510142",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ChatPromptTemplate: Creates a chat template consisting of a single message assumed to be from the human (https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "816a8bca-169b-49da-b46e-7d9f90181ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c29f126-a3a0-47f9-889b-a0957dd170eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18fa10e1-8eb4-433d-a0f7-c20d3d969db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e01c18bc-c491-4f2f-a54b-ad77ccd45a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65261d94-24dc-451c-8c26-54056ca8cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f578d387-5b80-49f1-831d-b1a76a3ae424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "351d037c-7fce-420b-9c4c-14add2d6cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2ee53cf-f453-4437-8593-c9c2963b6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c128dd-c318-4131-b2e1-489704b348f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30d99108-d4ac-40b0-9290-70461f254427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4275cff-f4db-4c47-bd5a-1cdbe5be29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81a47055-4127-4341-ad8f-971d479ec1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec0fd6f-6978-4352-a85b-34a89f9993e2",
   "metadata": {},
   "source": [
    "## Get embeddings model (SentenceBERT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f462fa-2cea-49fb-9579-57944e59c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sentence BERT for sentence embeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0af948-e33d-42b8-bddc-64eea02f2973",
   "metadata": {},
   "source": [
    "## Get embeddings model (Nomic embddings)\n",
    "## Original : https://blog.nomic.ai/posts/nomic-embed-text-v1, https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf\n",
    "## https://huggingface.co/nomic-ai/nomic-embed-text-v1\n",
    "## https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceBgeEmbeddings.html#langchain_community.embeddings.huggingface.HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "021e5190-2fe5-4ab4-bfef-a271fb70fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nomic-embed-text-v1\tis 8192 seq len embedder that is open source\n",
    "model_name = \"nomic-ai/nomic-embed-text-v1\"\n",
    "model_kwargs = {\n",
    "    'device': 'cpu',\n",
    "    'trust_remote_code':True\n",
    "    }\n",
    "encode_kwargs = {'normalize_embeddings': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2684cb69-8383-461a-987a-05e9d84180dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hardiksahi/miniconda3/envs/rag_env/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/Users/hardiksahi/miniconda3/envs/rag_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, query_instruction=\"search_query:\", embed_instruction=\"search_document:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaf67404-154b-4126-aae0-0fb7bf2b5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921db44f-9466-48a5-95ef-4cbd89939ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of question_embedding: 768\n",
      "Dimensionality of document_embedding: 768\n"
     ]
    }
   ],
   "source": [
    "question_embedding = embeddings.embed_query(question)\n",
    "document_embedding = embeddings.embed_query(document)\n",
    "print(f\"Dimensionality of question_embedding: {len(question_embedding)}\")\n",
    "print(f\"Dimensionality of document_embedding: {len(document_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506cd46d-5025-4f43-9eb1-91d27f743640",
   "metadata": {},
   "source": [
    "## Define similarity metric (cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "918c90dc-ef9c-4c70-bed6-be66e964af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product/(norm_vec1*norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55252c68-7ad8-4ede-b024-09edcb053fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between questiona dn document: 0.7388467122620958\n"
     ]
    }
   ],
   "source": [
    "sim = cosine_similarity(question_embedding, document_embedding)\n",
    "print(f\"Cosine Similarity between questiona dn document: {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda394d-9fd1-4297-ada2-92f0b5a8c04e",
   "metadata": {},
   "source": [
    "## Get token count (as per BPE implemented in tiktoken library by OpenAI). Does not make sense for SentenceBert embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4fc4c9f-f9c9-4919-a6c2-811121866bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_token_number(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4892ea6-db9a-4919-81b5-3d6a5db34b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## cl100k_base is GPT-4 tokenizer\n",
    "count_token_number(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4359c0-0ee1-4267-b080-1bba0fe600c7",
   "metadata": {},
   "source": [
    "## Visualize tokenization done by cl100k_base (GPT-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "528ab1e3-7171-4560-b166-5d52ac1a77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;167mW\u001b[48;5;179mh\u001b[48;5;185ma\u001b[48;5;77mt\u001b[0m\n",
      "\u001b[48;5;167mW\u001b[48;5;179mh\u001b[48;5;185mat\u001b[0m\n",
      "\u001b[48;5;167mWh\u001b[48;5;185mat\u001b[0m\n",
      "\u001b[48;5;167mWhat\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mk\u001b[48;5;185mi\u001b[48;5;77mn\u001b[48;5;80md\u001b[48;5;68ms\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mk\u001b[48;5;185min\u001b[48;5;80md\u001b[48;5;68ms\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mk\u001b[48;5;185mind\u001b[48;5;68ms\u001b[0m\n",
      "\u001b[48;5;167m k\u001b[48;5;185mind\u001b[48;5;68ms\u001b[0m\n",
      "\u001b[48;5;167m kind\u001b[48;5;68ms\u001b[0m\n",
      "\u001b[48;5;167m kinds\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mf\u001b[0m\n",
      "\u001b[48;5;167m o\u001b[48;5;185mf\u001b[0m\n",
      "\u001b[48;5;167m of\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mp\u001b[48;5;185me\u001b[48;5;77mt\u001b[48;5;80ms\u001b[0m\n",
      "\u001b[48;5;167m p\u001b[48;5;185me\u001b[48;5;77mt\u001b[48;5;80ms\u001b[0m\n",
      "\u001b[48;5;167m p\u001b[48;5;185met\u001b[48;5;80ms\u001b[0m\n",
      "\u001b[48;5;167m p\u001b[48;5;185mets\u001b[0m\n",
      "\u001b[48;5;167m pets\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179md\u001b[48;5;185mo\u001b[0m\n",
      "\u001b[48;5;167m d\u001b[48;5;185mo\u001b[0m\n",
      "\u001b[48;5;167m do\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mI\u001b[0m\n",
      "\u001b[48;5;167m I\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179ml\u001b[48;5;185mi\u001b[48;5;77mk\u001b[48;5;80me\u001b[0m\n",
      "\u001b[48;5;167m l\u001b[48;5;185mi\u001b[48;5;77mk\u001b[48;5;80me\u001b[0m\n",
      "\u001b[48;5;167m l\u001b[48;5;185mi\u001b[48;5;77mke\u001b[0m\n",
      "\u001b[48;5;167m li\u001b[48;5;77mke\u001b[0m\n",
      "\u001b[48;5;167m like\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m?\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3923, 13124, 315, 26159, 656, 358, 1093, 30]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = SimpleBytePairEncoding.from_tiktoken(\"cl100k_base\")\n",
    "enc.encode(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc58711-0b63-464a-b89d-38a19c058b92",
   "metadata": {},
   "source": [
    "## INDEXING (load, split and embed documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2033c3-1e5d-4787-a8c9-98e1b87f0cc6",
   "metadata": {},
   "source": [
    "## 1. Load data from web page and use Beautifuloup to parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41d1538c-ae52-4f20-aa07-f6c1b3fd34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d375bb-a261-4853-9465-2e240bfba581",
   "metadata": {},
   "source": [
    "## 2. Split data using chunking strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7489cc8-e988-4001-acc2-9fc952972122",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7d9c81a-5228-419d-a7c5-ec09395d994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88531a30-af2b-45db-ab72-751fcc9eb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits from blog_docs: 52 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of splits from blog_docs: {len(splits)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8402a-9c7c-4454-8328-f6e5fdb5bfc2",
   "metadata": {},
   "source": [
    "## SPlits doc into splits. Each split has 300 tokens with overlap of 50 tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93f404-d33e-4c9a-b0bf-52621a8a592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_length_list = [len(sp.page_content) for sp in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218df6fc-c099-4004-a883-f956a2460612",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=content_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2c2e4-a045-40c9-8559-ab976f0c2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metadata is the extra information around the doc that is split using recursive splitter\n",
    "splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077ab90-2fa2-4c13-acbb-992ecc0fd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818cbfd-e0c7-493c-9a69-8c57d0e502e9",
   "metadata": {},
   "source": [
    "## 3. Store embeddings to chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f2dce-4cbf-46fb-aeb1-764a686255cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize chroma db client\n",
    "vectorstore = Chroma.from_documents(collection_name=\"rag_store_nomic\", persist_directory=\"notebooks/chroma\", documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfdfa5-dab1-4bc7-983a-a754ccb04d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.get('043be940-5eb3-4ca9-ae66-6d07e094b493')#.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fe4c8-bf01-4fda-90bf-adc271dbe554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d6741-f214-48fc-a75d-390d2d0e4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreiver = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs = {\"k\":4})\n",
    "\n",
    "## This will prevent from getting unsure documents\n",
    "retreiver = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.35}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacaf02a-38e0-4b83-97f2-b66ae49da5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = await retreiver.ainvoke(\"What is task decoposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def08824-6e96-4b72-8377-ffe5665fac57",
   "metadata": {},
   "source": [
    "## These retreived splits are embedded into the context of LLM(GPT or LLama) as prompt to answer the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87318109-6029-4dc4-942d-a1dc0a6c34b0",
   "metadata": {},
   "source": [
    "## 4. Create prompt (https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b9da8-54c8-49fb-b40c-afa74d1e4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0cb68-e585-437c-8fc5-1b3db88fd311",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89d940-4e79-433d-9b48-60de87a47960",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a52780-38ba-4c44-9450-549fd9b53260",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = ChatOllama(model=\"llama2:latest\") ## num_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432561c-3881-4c59-8046-ae990707c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = {\"context\":retreiver, \"question\": RunnablePassthrough()} | prompt | local_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dabf5e-3586-4648-807b-6cdfdc41dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = prompt | local_llm\n",
    "\n",
    "# chain.invoke({\"context\":relevant_docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8bd67c-b7af-4df8-bf7b-da90a867663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is task decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def80f95-8fe7-469c-9e83-1d249accdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"Summarize algorithm distillation for me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b6cf9-79b9-482d-b222-1ea370fa6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What are the different types of agent memory?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06029860-aeb8-426e-9a96-cdefb4090cd5",
   "metadata": {},
   "source": [
    "## Updated prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54008cc1-894b-4d39-80ca-b721db577ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7b927-8d8f-4cdc-a1dd-88b9ad66b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_rag_chain = {\"context\":retreiver, \"question\": RunnablePassthrough()} | prompt_hub_rag | local_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746d3e92-28bd-4160-96cd-a62750148da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is from the pretrained knowledge of Llama2. It is not in the context\n",
    "updated_rag_chain.invoke(\"What can you say about training of large language models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b8fdb-7da2-4ad4-adce-cf7d5983eecd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec8318-68f0-4441-b504-a16a654004a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs1 = await retreiver.ainvoke(\"What can you say about Narendra Modi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55a101-2114-4d9d-b9c4-370f124d2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2c01d-0134-4509-8dfb-a254ab30178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_rag_chain.invoke(\"What can you say about Narendra Modi?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca2228-7457-4940-8765-44013942914a",
   "metadata": {},
   "source": [
    "# Part2: https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae81b1b-15a5-4493-ad59-a789bb1b3c73",
   "metadata": {},
   "source": [
    "# QUERY TRANSLATION: Rewriting the query input by the user in a way that makes retreival easier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e0a6a-a110-4fb3-ac99-6bf2c9469c0c",
   "metadata": {},
   "source": [
    "### Way1: MultiQuery:\n",
    "1. Break input query into multiple queries from different perspectives (Ask the LLm to do so)\n",
    "2. Retreive documents for each of the above queries parallely\n",
    "3. Union of documents returned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd158ce-4c07-4f00-ae21-dc286d5f3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191dc98-8c1b-43e1-afa1-3659b2037b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_perspectives = ChatPromptTemplate.from_template(multiquery_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24f615-42a6-4ab0-b978-ceb7c12670e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7039c-b47a-42bb-a05d-f3f1dc5b1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_related_queries_chain = prompt_perspectives | local_llm  | StrOutputParser() | (lambda x: x.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c4f1e-2985-4021-959b-3095b5fe1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_questions = await generate_related_queries_chain.ainvoke({\"question\": \"What is task decomposition for LLM agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00d5f7-4fd6-4321-8b74-333466cc5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1474af3c-acad-41b2-bb4d-0d71081b39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(documents: list[int]):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5327b2-1d7d-44ae-b85e-88fc937dd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_retreival_chain = generate_related_queries_chain | retreiver.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43cc2f6-381c-475f-b3d1-e2e0d603a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_question = \"What is task decomposition for LLM agents?\"\n",
    "multiquery_docs = multiquery_retreival_chain.invoke({\"question\":multiquery_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb385a1-0ec4-4e04-a8fc-3d23f3f6a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multiquery_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde8347-b8c7-409b-a316-d02cd6be51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_rag_chain = {\"context\":multiquery_retreival_chain, \"question\": RunnablePassthrough()} | prompt_hub_rag | local_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8a1d0-930b-476e-aaaa-aad85edb61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiquery_rag_chain.invoke(multiquery_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a103d17-b548-43e4-89bb-8589ee1daeb0",
   "metadata": {},
   "source": [
    "## My understanding is mutliquery query translation makes sense when the question is very broad since it breaks initial question into queries from different perspectives. This might be problematic when initial questions are very pointed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e44ae9-53b5-4027-ab31-14efb19c993a",
   "metadata": {},
   "source": [
    "### Way2: RAG-Fusion\n",
    "1. Break query into different queries from varying perspectives\n",
    "2. Ask LLM to answer these queries parallely\n",
    "3. Combine the answers using a special technique called reciprocal rank fusion (RRF) \n",
    "4. Return the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5716e-f753-404d-b48f-50bcc933c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    ## results is a list of lists of size 4 (# subqueries). Each element is a list of retreived documents\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(docs) ## Convert doc to string\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            #previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str]+=1/(rank+k)\n",
    "\n",
    "    reranked_results = [(loads(doc), rrf_score) for doc, rrf_score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb1cc95-a88a-499c-8738-eb6ee39f4f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_for_splitting_single_query = \"\"\"You are a helpful assistant that generates multiple search queries based on single input query.\n",
    "Generate multiple search queries related to : {question}\n",
    "Output 4 queries:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73e77f-3e4a-4a93-b4f0-09d0ac3d3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_splitting_single_query = ChatPromptTemplate.from_template(template_for_splitting_single_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3e1d7-17d1-418b-adfc-d87886ff4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cc = {\"question\": RunnablePassthrough()} | prompt_splitting_single_query\n",
    "#cc.invoke(\"how to train LLM?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6444e6-d101-433d-b698-05bee721176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_chain = prompt_splitting_single_query | local_llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00adfb10-e63c-4f69-8f9d-f3615f858d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generates 4 related queries\n",
    "#generate_queries_chain.invoke({\"question\": \"how to train LLM?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd3327-df81-4c3d-86fc-028a922fe71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_fusion_retreival_chain = generate_queries_chain | retreiver.map() | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1fdf80-9e97-4b91-84ef-cf3355d1287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2844c8-f0ef-48e8-babd-1355aa9aff86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "returned_docs_as_per_rrf = await rag_fusion_retreival_chain.ainvoke({\"question\": \"what can you tell me about chain of thought?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fc744-8496-452d-be15-a99fc4c07fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_for_generation = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa4746-3b96-4363-9d69-c00c155d1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_generation = ChatPromptTemplate.from_template(template_for_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87fff73-0475-4b4c-9a0a-2f4a5a936040",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_fusion_generation_chain = {\"context\":rag_fusion_retreival_chain, \"question\": itemgetter(\"question\")} | prompt_for_generation | local_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014767a-e250-4b7b-b9cf-005ac4053eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rag_fusion_response = await final_rag_fusion_generation_chain.ainvoke({\"question\":\"What is task decomposition for LLM agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64d994-80ff-4d51-9c54-026f5cfa24d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_fusion_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64902b15-6e4f-4970-9ff4-ed8a44da6837",
   "metadata": {},
   "source": [
    "### Way 3: Query decomposition using Least to Most prompting and IR-CoT (Information retreival with Chain of Thought)\n",
    "Note: \n",
    "- Chain of Thought prompting combines natural language based rationale with few shot prompt. (https://arxiv.org/pdf/2201.11903)\n",
    "- CoT is further improved by adding self-consistency decoder (opposed to greedy decoder in vanilla CoT) (https://arxiv.org/pdf/2203.11171)\n",
    "- However, CoT has limitations that it performs poorly on tasks that require generalization of solving problems harder than few shot prompt examples.This is where Least to Most prompting comes in picture\n",
    "\n",
    "Least to Most prompting:\n",
    "1. Decompose a query into easier subqueries\n",
    "2. Sequentially solve subqueries using the reponse/ answer to previous subqueries\n",
    "3. Both stages are implemented by few-shot prompting, so that there is no training or\n",
    "finetuning in either stage\n",
    "\n",
    "IR-CoT (Information retreival with Chain of Thought): (https://arxiv.org/pdf/2212.10509)\n",
    "1. How can we augment chain-of-thought prompting for open-domain, knowledge-intensive tasks that require complex, multi-step reasoning?\n",
    "2. Use retrieval to guide the chain-of-thought (CoT) reasoning steps and use CoT reasoning to guide the retrieval.\n",
    "Steps:\n",
    "a. We begin by retrieving a base set of paragraphs using the question as a query.\n",
    "b. Subsequently, we alternate between the following two steps: (i) extend CoT: use the question, the paragraphs collected thus far, and the CoT sentences generated thus far to generate the next CoT sentence; (ii) expand retrieved information: use the last CoT sentence as a query to retrieve,additional paragraphs to add to the collected set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdefdf-c9a7-41f1-9f0a-d47c9b5f371c",
   "metadata": {},
   "source": [
    "## So, Query decomposition means break query into simpler subproblems and then dynamically retreive answers for smaller problem and use to answer next subproblem.Useful only if it makes sense to decompose query into sub problems. e.g. complicated reasoning question where answer of simpler query will help answer more complicated query. else this approach is an overkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03b4ca-1940-4562-8f64-887ba758ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpler_subproblems_template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. Only return the sub problems, nothing else \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_simpler_subproblems_query = ChatPromptTemplate.from_template(simpler_subproblems_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47711e4-0418-4151-8b77-801fe5d11585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simpler_subproblems_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7696fd-a5c8-4870-bb93-8542b4f5427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_decomposition_chain = prompt_simpler_subproblems_query | local_llm | StrOutputParser() | (lambda x: [ss for ss in x.split(\"\\n\") if ss.strip() != '' and ss[0].isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d585aa7-f29f-4731-8af5-510f8e5738ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decomposed_questions = await generate_queries_decomposition_chain.ainvoke({\"question\": \"What are the main components of an LLM-powered autonomous agent system?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45029ba8-9eef-424e-89e3-be720b5a6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a09e4a-62f7-497b-a104-9245cfbbc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_cot_template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "prompt_ir_cot = ChatPromptTemplate.from_template(ir_cot_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44f9b0-41d5-4ed8-9820-2cb4a1199592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436fa51d-4b4d-4b0f-a5e4-8a4c4646dc64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q_a_pairs = \"\"\n",
    "for question in decomposed_questions:\n",
    "    ## Retreive docs relevant to question (context) + all previously answered questions (q_a_pairs) and answer current question\n",
    "    rag_chain = {\"context\": itemgetter(\"question\") | retreiver, \"question\": itemgetter(\"question\"), \"q_a_pairs\": itemgetter(\"q_a_pairs\")} | prompt_ir_cot | local_llm | StrOutputParser()\n",
    "\n",
    "    answer = await rag_chain.ainvoke({\"question\": question, \"q_a_pairs\": q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(question, answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\" + q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5ff70-9dd2-417c-9505-db288007b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d3165-24bb-4164-8b7a-e80c03c6ffec",
   "metadata": {},
   "source": [
    "### Way 4: Step back prompting: Useful for knowledge intensive tasks where it will benifit from thinking from first principles/ overall level/ step back to get broader picture.\n",
    "- Step back question: derived from original question at a higher level of abstraction (This step back question should be much easier to answer)\n",
    "\n",
    "2 steps:\n",
    "1. Abstraction: Get step back question and retreive relevant concepts for it.. This step back question is task dependent.\n",
    "2. Reasoning (Abstraction grounded reasoning): Answer original question but stay grounded on facts obtained from Abstraction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f63553-7077-4bb7-83f7-82339f2bed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 shot examples\n",
    "## input: query, output: step back version of query\n",
    "examples = [{\"input\": \"Could the members of The Police perform lawful arrests?\", \"output\": \"what can the members of The Police do?\"}, {\"input\": \"Jan Sindel’s was born in what country?\", \"output\": \"what is Jan Sindel’s personal history?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a945e-7d39-4f58-a7f6-7ba71dbc420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages([(\"human\", \"{input}\"), (\"ai\", \"{output}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a590510-57b0-41f5-b2f7-6c9c77697524",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(example_prompt=example_prompt, examples=examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253bba5e-4744-442a-b86c-56d1893bb1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_prompt = ChatPromptTemplate.from_messages([(\"system\", \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\"), few_shot_prompt, (\"user\", \"{question}\")]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e27dc0d-c789-483e-9f61-cc9e491b05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_prompt.invoke({\"question\": \"What is task decomposition for LLM agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72314412-066a-4021-a45a-0396d64e41f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_step_back_queries_chain = step_back_prompt | local_llm | StrOutputParser()\n",
    "step_back_query = await generate_step_back_queries_chain.ainvoke({\"question\": \"What is task decomposition for LLM agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe368b2-5df0-4c4d-92e0-7aeb4956820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_back_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b935875-d004-4b35-aa3c-8ae432b930b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_response_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "{normal_context}\n",
    "{step_back_context}\n",
    "\n",
    "Original Question: {question}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42189447-899c-4dda-a79b-e8378298d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_response_prompt = ChatPromptTemplate.from_template(step_back_response_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d503a-f731-4542-9547-e9ef582d3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_response_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22127b-f0f2-4aec-b120-ffbfb6b2f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_chain = {\"question\": itemgetter(\"question\"), \"normal_context\": itemgetter(\"question\") | retreiver, \"step_back_context\": generate_step_back_queries_chain | retreiver } | step_back_response_prompt | local_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb98f19-383b-48cf-95e6-8c200cb392f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step_back_answer = await step_back_chain.ainvoke({\"question\": \"What is task decomposition for LLM agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e4033-bc06-45ee-8dea-c91263acaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(step_back_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa6c1b-1c31-4677-b8e4-8e0ef70aa5d8",
   "metadata": {},
   "source": [
    "### Way 5: HyDE (Hypothetical Document Embedding) => https://docs.haystack.deepset.ai/docs/hypothetical-document-embeddings-hyde\n",
    "1. Given a query, the Hypothetical Document Embeddings (HyDE) first zero-shot prompts an instruction-following language model to generate a “fake” hypothetical document that captures relevant textual patterns from the initial query - in practice, this is done five times. (basically ask Instruction following LLM to generate paragraphs to answer user query 5 times)\n",
    "2. Embed the 5 results from Step 1 using the same embedder as the one used to embed docs and save in Chroma DB\n",
    "3. Average the embeddings (for 5 documents) to get a single Hypothetical Document embedding\n",
    "4. Now, search top_k documents with embedding similar to the hypothetical document created in previous step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d335599-a777-4f9d-99cb-f629427f18fa",
   "metadata": {},
   "source": [
    "When to use?\n",
    "The HyDE method is highly useful when:\n",
    "\n",
    "1. The performance of the retrieval step in your pipeline is not good enough (for example, low Recall metric).\n",
    "2. Your retrieval step has a query as input and returns documents from a larger document base.\n",
    "3. Particularly worth a try if your data (documents or queries) come from a special domain that is very different from the typical datasets that Retrievers are trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170fe09-48a2-402e-b8d1-19c6f3da864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_prompt_template = \"\"\"Given a question, generate a paragraph of text that answers the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "hyde_prompt = ChatPromptTemplate.from_template(hyde_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef07de7-318c-47c6-9b49-681a9042a1e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_hypothetical_docs = 5\n",
    "hypothetical_doc_list = []\n",
    "for c in range(count_hypothetical_docs):\n",
    "    hyde_chain = {\"question\": itemgetter(\"question\")} | hyde_prompt | local_llm | StrOutputParser()\n",
    "    hyp_doc = await hyde_chain.ainvoke({\"question\": \"What is task decomposition for LLM agents?\"})\n",
    "    hypothetical_doc_list.append(hyp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196374c0-71c1-4c48-81bd-b4dcdb68c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_document_embedding_list = await embeddings.aembed_documents(hypothetical_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82501451-bd0b-40d1-89ae-7c2a7757bcd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypothetical_embedding_matrix = np.array(hypothetical_document_embedding_list)\n",
    "average_hypothetical_document_embedding = np.mean(hypothetical_embedding_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca7c07-906c-423d-a49a-0a0f744533e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_hypothetical_document_embedding_list = average_hypothetical_document_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c712b-51e3-446e-b337-f075e5fd3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now find tok_k relvant docs for this query\n",
    "## https://api.python.langchain.com/en/v0.1/vectorstores/langchain_community.vectorstores.chroma.Chroma.html#langchain_community.vectorstores.chroma.Chroma.asimilarity_search_by_vector\n",
    "hyde_relevant_documents = await vectorstore.asimilarity_search_by_vector(average_hypothetical_document_embedding_list, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82b31a-506f-4055-abfd-e025281e5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hyde_generation_template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "final_hyde_generation_prompt = ChatPromptTemplate.from_template(final_hyde_generation_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448736ef-a109-4cec-9ae6-372ff6f467d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hyde_generation_chain = {\"context\": itemgetter(\"context\"), \"question\": itemgetter(\"question\")} | final_hyde_generation_prompt | local_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c740e9a-5f5e-43c5-b58a-d10d600e0215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hyde_generated_response = await final_hyde_generation_chain.ainvoke({\"context\": hyde_relevant_documents, \"question\": \"What is task decomposition for LLM agents?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be10907-3d71-4dc4-9889-4c1ef899a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyde_generated_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065d82b-9291-4216-ad09-6f52423e5b65",
   "metadata": {},
   "source": [
    "# Part3: https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_10_and_11.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29864f96-fb78-4149-873c-3ac873f149e1",
   "metadata": {},
   "source": [
    "# ROUTING: Get query to the correct source. 2 types:\n",
    "1. Logical routing: Route query to correct retreiver/ database e.g. vecotr db, graph db etc (via structured functions as LLM output). Consider it as a classification system that given the query, returns the db system to be used for query answering\n",
    "2. Semantic routing: Embed multiple prompts and query using same embedder and choose the prompt that has highest similarity with query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fef34e-a562-4c53-8c3d-ef25ba73763e",
   "metadata": {},
   "source": [
    "## 1. Logical routing: Use function-calling for classification (LLM for classification with structured output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1501cf-3612-4d5c-b20d-4d59ab426c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Create a class/ data model that will be the output of LLM\n",
    "\n",
    "# class RouteQuery(BaseModel):\n",
    "#     datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(description=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102eac2-81cb-4d16-8d6b-87650d6b9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understandinfg use of pydantic to create Data model to which the output of LLM should confootm to\n",
    "## https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/pydantic/\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to setup a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "joke_query = \"Tell me a joke\"\n",
    "joke_parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "joke_prompt = PromptTemplate(template = \"Answer the user query. \\n{format_instructions}\\n{query}\\n\", input_variables=[\"query\"], partial_variables={\"format_instructions\": joke_parser.get_format_instructions()})\n",
    "#structured_local_llm = local_llm.with_structured_output(BaseModel)\n",
    "joke_chain = joke_prompt | local_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ded02-e4da-40c8-9dca-b0b366cc39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#joke_prompt.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46a934-e29f-4b75-ad26-511d4df332bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joke_template = \"\"\"You are a great comedian who makes scientific jokes. You tell jokes according to the question asked below.\\n\n",
    "# question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "# simple_joke_chain = {\"question\": RunnablePassthrough()} | ChatPromptTemplate.from_template(joke_template) | local_llm |StrOutputParser()\n",
    "\n",
    "# simple_joke = simple_joke_chain.invoke(\"Tell me a joke related to plants\")\n",
    "\n",
    "# print(simple_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5b910-de7f-4cdd-9ab3-f6e64ef45152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = joke_chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719d6a62-39db-43aa-a250-e0be1f83cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05111181-ab4b-4ada-ad73-84403a33b45a",
   "metadata": {},
   "source": [
    "## LLama 2 model is not able to follow the datamodel. Atleast that is what I could understand. This might work with ChatGPT based LLMs.\n",
    "## Check https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/#groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1346e7a-eda7-4e49-85a0-df30f94cea80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Doing function calling with open source local models like llama are experimental in langchain till now.\n",
    "## https://api.python.langchain.com/en/latest/llms/langchain_experimental.llms.ollama_functions.OllamaFunctions.html#langchain_experimental.llms.ollama_functions.OllamaFunctions\n",
    "## Video to explain function calling in LLama models: https://www.youtube.com/watch?v=Ss_GdU0KqE0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9effad5-7d64-457b-a75b-4369c5f73347",
   "metadata": {},
   "source": [
    "## Experiment with Llama and Phi3 models for function calling: https://export.arxiv.org/pdf/2404.14219 in FunctionCallingWithLocalModels notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9d934-a26e-4210-87b7-254baaf88e62",
   "metadata": {},
   "source": [
    "## Implement logical routing using langchain_experimental.llms.ollama_functions.OllamaFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783072f2-3357-4b8a-8ccb-25877c60ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(description=\"Given a user question choose which datasource would be most relevant for answering their question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02792fb7-c6e8-4092-81e3-7b20412234d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_local_llm = OllamaFunctions(model=\"llama2:latest\", format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787452a-915e-4301-ac68-6928ce915678",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_experimental_local_llm = experimental_local_llm.with_structured_output(RouteQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c7576-26bc-42ec-bd0f-aa2371d5368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_messages = [\n",
    "    SystemMessage(content=\"You are an expert at routing a user question to the appropriate data source. Based on the programming language the question is referring to, route it to the relevant data source.\"),\n",
    "    HumanMessage(content=\"{query}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc01fc-80a4-4a92-b411-5c79b15f0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_prompt = ChatPromptTemplate.from_messages(routing_messages)\n",
    "print(routing_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0cc91f-f30d-4054-8383-b083c774f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "router = routing_prompt | structured_experimental_local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9c0c5-8d07-46c8-bb20-6fe0d9cd1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586260b-d606-4946-a2f3-6e9fc04030d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "router_result = router.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf88a34-8823-4dac-83a6-0bd5e72146ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "RouteQuery.parse_obj(router_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049cc44-54df-47e6-837d-09f0bd00c77d",
   "metadata": {},
   "source": [
    "## It appears as if Llama 2 model is not able to distinguish between coding languages. It is possible that it is not trained on code. But the concept of making LLM output confirm to a json/ data model is very critical concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2372db-efd9-4c78-b638-78b2137324a7",
   "metadata": {},
   "source": [
    "## 2. Semantic routing: Choosing between multiple prompts based on similarity with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5f747-4c36-4c59-bb55-fc4bb8966291",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea5f68-5595-44f6-bca2-1a4d44b16219",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_prompt_templates = [physics_template, math_template]\n",
    "multiple_prompt_embeddings = await embeddings.aembed_documents(multiple_prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2c3cb-9292-48a7-b7ec-29b93b527235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_router(input_dict):\n",
    "    query_embedding = embeddings.embed_query(input_dict[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], multiple_prompt_embeddings)[0]\n",
    "    most_similar = multiple_prompt_templates[similarity.argmax()]\n",
    "    print(f\"Using Math termplate\" if most_similar == math_template else \"Using Physics template\")\n",
    "    return ChatPromptTemplate.from_template(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5526b-4a50-4381-b66c-93447fdc4c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "semantic_router_chain = {\"query\": RunnablePassthrough()} | RunnableLambda(prompt_router) | local_llm | StrOutputParser()\n",
    "semantic_router_result = semantic_router_chain.invoke(\"Explain to me special theory of relativity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f6d47a-b665-40f6-aa69-6339bdab705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantic_router_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52147fb3-16fd-42e2-a4fe-a2a3ed621ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maths_router_result = semantic_router_chain.invoke(\"Explain me the concept of imaginary numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6eecd-b1db-4343-9508-02d59ef73b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maths_router_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a915b-15e3-4621-9972-0c30d7d921f1",
   "metadata": {},
   "source": [
    "# QUERY CONSTRUCTION: Based on the database used (vectorDb/ GraphDb etc), translate human language query into form suitable to be run on dbase.\n",
    "## e.g. If dbase used is Vector DB, extract metadata filter information from the input query and run them against the vector dbase like Chroma\n",
    "## https://python.langchain.com/v0.2/docs/tutorials/query_analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f2180-8dd1-4f60-9ed2-50b0906a9a5d",
   "metadata": {},
   "source": [
    "## In this example we will use youtube video transcripts etc as document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfbd45-de69-436d-a17f-5019bb583ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_urls = [\n",
    "    \"https://www.youtube.com/watch?v=HAn9vnJy6S4\",\n",
    "    \"https://www.youtube.com/watch?v=dA1cHGACXCo\",\n",
    "    \"https://www.youtube.com/watch?v=ZcEMLz27sL4\",\n",
    "    \"https://www.youtube.com/watch?v=hvAPnpSfSGo\",\n",
    "    \"https://www.youtube.com/watch?v=EhlPDL4QrWY\",\n",
    "    \"https://www.youtube.com/watch?v=mmBo8nlu2j0\",\n",
    "    \"https://www.youtube.com/watch?v=rQdibOsL1ps\",\n",
    "    \"https://www.youtube.com/watch?v=28lC4fqukoc\",\n",
    "    \"https://www.youtube.com/watch?v=es-9MgxB-uc\",\n",
    "    \"https://www.youtube.com/watch?v=wLRHwKuKvOE\",\n",
    "    \"https://www.youtube.com/watch?v=ObIltMaRJvY\",\n",
    "    \"https://www.youtube.com/watch?v=DjuXACWYkkU\",\n",
    "    \"https://www.youtube.com/watch?v=o7C9ld6Ln-M\",\n",
    "]\n",
    "youtube_docs = []\n",
    "for url in youtube_urls:\n",
    "    youtube_docs.extend(YoutubeLoader.from_youtube_url(url, add_video_info=True).load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725159e-1e6d-44db-8e36-31375dd95235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc in youtube_docs:\n",
    "    doc.metadata[\"publish_year\"] = int(datetime.datetime.strptime(doc.metadata[\"publish_date\"], \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6a04f-b8cf-45d6-9781-5e35d5afec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc.metadata[\"title\"] for doc in youtube_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54ed93-ddb7-4a46-8a71-855b95797df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[doc.metadata[\"view_count\"] for doc in youtube_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644480c-1a46-4f4f-b76d-f4dbe20eb64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24950ad1-e4b8-4ee7-bbdb-80d866b0ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chunk youtube docs using RecursiveCharacterTextSplitter, use nomic embedder to get embeddings and store it into a vector database (Chroma)\n",
    "youtube_splits = splitter.split_documents(youtube_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511f235-9f9b-41f8-a1d6-104e9fa19fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_vectorstore = Chroma.from_documents(collection_name=\"youtube_store_nomic\", documents=youtube_splits, embedding=embeddings, persist_directory=\"./chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d141bd8-7e4f-4bd2-af56-cf582cdf8d65",
   "metadata": {},
   "source": [
    "## Now create a BaseModel that helps extract relevant metadata from input query that can then be applied on top on chroma db (youtube_vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3e24d-bd09-46bf-aca8-693596f272dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutorialSearch(BaseModel):\n",
    "    content_search: str = Field(description=\"Similarity search queries that can be applied to video transcripts\")\n",
    "    title_search: str = Field(description=\"Succinct title with only the keywords\")\n",
    "    min_view_count: Optional[int] = Field(description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\")\n",
    "    max_view_count: Optional[int] = Field(description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\")\n",
    "    publish_year: Optional[int] = Field(description=\"Year when the video was published. Use only if explicitly specified\")\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee70b1-c130-4720-a865-12f3847dbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_experimental_youtube_local_llm = experimental_local_llm.with_structured_output(TutorialSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded55125-105c-4ae3-94b9-60ff5b9d244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube_messages = [\n",
    "#     SystemMessage(content=\"\"\"You are an expert at converting user questions into database queries.\n",
    "#     You have access to a database of tutorial videos about a software library for building LLM-powered applications.\n",
    "#     Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "#     If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"),\n",
    "#     HumanMessage(content=\"{question}\")\n",
    "# ]\n",
    "\n",
    "# youtube_prompt = ChatPromptTemplate.from_messages(youtube_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2355034-14f6-406b-8c80-b24e726a505f",
   "metadata": {},
   "source": [
    "## Need to ensure that the prompt matches the template that was used during LLama2 training\n",
    "## Check https://ollama.com/library/llama2/blobs/2e0493f67d0c for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f941e34-9c75-4b96-b8c2-bdcc7100e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"[INST] <<SYS>>\n",
    "    {system_prompt}\n",
    "    <</SYS>>\n",
    "    {question} [/INST]\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddc2ab-f233-478f-af52-868be154322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a5907-167d-4d3e-911a-692630dee0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_constructor_chain = youtube_prompt | structured_experimental_youtube_local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba95298-9c8d-44cb-9f0e-ad282a744c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_llama2_system_prompt = \"\"\"You are an expert at converting user questions into database queries.\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications.\n",
    "Given a question, return a database query optimized to retrieve the most relevant results.\n",
    "\n",
    "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d294d-4698-4c76-ad58-4b80adcc9eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "youtube_resp = query_constructor_chain.invoke({\"system_prompt\": youtube_llama2_system_prompt, \"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes published in 2021\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636785d1-0740-4413-92cf-1531d3e772e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24597f0a-bec1-41b0-b687-ec2b24abfbc6",
   "metadata": {},
   "source": [
    "## LLama 2 does not appear to have the capability to extract elevant information from the query in the above case. It is worth experimenting with other LLMs that are trained specifically for function calling. For example:\n",
    "1. Phi models\n",
    "2. Nexusravn: https://ollama.com/library/nexusraven/blobs/cf200ab0155f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9e980-78d8-4fba-8048-2b4050c5e6cb",
   "metadata": {},
   "source": [
    "## https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.chroma.Chroma.html#langchain_community.vectorstores.chroma.Chroma.as_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3056fc-6ef3-40ff-a3eb-26e47602c7ea",
   "metadata": {},
   "source": [
    "## The above can also be done using json instead of DataModel using Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd9815e-701f-455e-8bed-fb7585c0daf7",
   "metadata": {},
   "source": [
    "# INDEXING: Ways to improve indexing in vector dbases\n",
    "### Way 1: Multi representation indexing => Take a document, ask LLM to summarize it and then index this summary in the vectorDB. It is possible only in case of LLMs with large context window. Otherwise, it is not possible.\n",
    "e.g. Llama2 model with 32K context (llama-2-7B-32K), Original LLama2 has 4K tokens as input context\n",
    "\n",
    "Basically change the granularity at which information is stored in Vector DB\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2312.06648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4b0e2-599c-4414-9c97-e30f7569c6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "rag_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
